% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{multirow}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Open-Set Domain Adaptation through Self-Supervision}

\author{Protopapa Andrea, Quarta Matteo, Ruggeri Giuseppe, Versace Alessandro\\
Politecnico di Torino\\
Italy\\
{\tt\small \{s286302,s292477,s292459,s292477\}@studenti.polito.it <- Riordinare}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  Performance loss over the change of domain in deep learning applications is a common problem.
  Solving the issues usually relies on adapting the model to the new domain having at disposal large and labeled datasets, which is not always the case or cost-effective.
  In this paper we propose a new approach to the problem, to adapt in a robust manner a model to the new domain while not needing large amounts of data thanks to a self-supervised task.
  Experiments conducted on the Office-Home dataset show really promising results.
\end{abstract}

\section{Introduction}
\label{sec:intro}
Classical Machine Learning in the past years have made some oversimplified assumptions actually detached 
from the usage of Artificial Intelligent systems in everyday real world and the problems they bring.

The firt assumption is that the training and test sets come from the same distributions.
Therefore, a model learned from the labeled training data is expected to perform well on the test data.
However, this assumption may not always hold in real-world applications where the training and the 
test data fall from different distributions and in this case naively applying the trained model on 
the new dataset may cause degradation in the performance. To solve this problem we can make use of 
Domain Adaptation, where our goal is to train a neural network on a source dataset for which labels 
are available, and test a good accuracy on the target dataset, which is related but
significantly different from the source dataset, and whose label or annotation is not available. 
Generally it seeks to learn a model from a source labeled data that can be generalized to a target domain
by minimizing the difference between domain distributions and enforcing feature extractor to extract 
similar features for source and target domain images.
As underlined in \cite{domainAdaptFarahani}, feature-based adaptation approaches aim to map the source data into the target data
by learning a transformation that extracts invariant feature representation across domains, transforming
the original features into a new feature space and then minimize the gap between domains in the new
representation space in an optimization procedure, while preserving the underlying structure of the
original data. 

Secondly, in real-world recognition/classification tasks it is usually difficult to collect 
training samples to exhaust all classes when training a recognizer or classifier.
A more realistic scenario is open set recognition (OSR) \cite{OSRsurvey}, where incomplete knowledge
of the world exists at training time, and unknown classes can be submitted to an algorithm
during testing, requiring the classifiers to not only accurately classify the seen classes
but also effectively deal with unseen ones, which otherwise drastically weakens the robustness
of the methods. On the contrary this system should reject unknown/unseen classes at test time and separate
the known and unknown samples.
As underlined by \cite{OSRclassRec}, existing open-set classifiers rely on deep networks trained in 
a supervised manner on known classes in the training set; this causes specialization of learned
representations to known classes and makes it hard to distinguish unknowns from knowns.

To solve these significant issues this method is focused on a self-supervised task. 
Self-supervised Learning is an unsupervised learning method where the supervised learning 
task is created out of the unlabelled input data. This task could be as simple as given 
the upper-half of the image, predict the lower-half of the same image.
Supervised learning requires usually a lot of labelled data and getting good quality labelled 
data is an expensive and time-consuming task. On the other hand, the unlabelled data is readily
available in abundance.
The fundamental idea for self-supervised learning is to create some auxiliary pre-text task
for the model from the input data itself such that while solving the auxiliary task,
the model learns the underlying structure of the data (for instance the high-level knowledge, correlations,
metadata embedded in the data).
This type of learning was recently used for Domain Adaptation, learning robust cross-domain features
and supporting generalization \cite{CarlucciJigsaw,SelfSupervisedXu}, and also for some Open Set problems specialized for anomaly detection
and discriminate normal and anomalous data \cite{bergman2020classificationbased,dectionGeometric}.

The approach presented in this paper brings these topics together in the so called Open-Set Domain 
Adaptation (OSDA) problem. A two-stage method is hence proposed, aiming to identify and isolate unknown
class samples in the first stage, before reducing in the second stage the domain shift between the source 
domain and the knwon target domain to avoid negative transfer.
All this is done in both stages using a modified version of the rotation task as self-supervised model,
predicting the relative rotation between an image and its rotated version.
Finally a classifier is used to predict if each target sample belongs either to one of the knwon classes
or to an unknown class, being in the latter case rejected.

The method was evaluated on the Office-Home benchmark \cite{venkateswara2017deep} with a specific OSDA metric.

ADD HERE RESULTS AND A BRIEF OF CONCLUSIVE IDEAS (ALSO POSSIBLE FUTURE WORKS)!!

%------------------------------------------------------------------------
\section{Related Work}
\label{sec:relatedwork}


\section{Method}
\label{sec:method}

\subsection{Problem Formulation}
\label{sec:problemformulation}
Our starting point is the source dataset, defined as $\mathcal{D}_s = \{({\bf x}_i^s, y_i^s)\}_{i=1}^{N_S} \sim p_s$, where each element ${\bf x}_i^s$ belonging to any $y_i^s$ is a sample from the source domain $S$.
This dataset has a target counterpart, $\mathcal{D}_t = \{{\bf x}_i^t\}_{i=1}^{N_t} \sim p_t$ which is unlabeled.
In OSDA we have that $p_s \neq p_t$.
The source dataset $\mathcal{D}_s$ is associated with a set of known classes, $\mathcal{C}_s$, which can also be found in the target dataset $\mathcal{D}_t$, but is supposedly smaller.
Hence we have that $|\mathcal{C}_s| < |\mathcal{C}_t|$ and that $\mathcal{C}_s \subset \mathcal{C}_t$.
In a setting of domain adaptation, we further have that $p_t^s \neq p_s$, the target distribution of the known source classes
A metric for measuring how different two domain are is the openness betweeen source and target domain\cite{bendale2015open}, defined as $\displaystyle \mathbb{O} = 1-\frac{\mathcal{C}_s}{\mathcal{C}_t}$.
When $\mathbb{O} > 0$, we're dealing with an OSDA problem.

\subsection{Approach}
\label{sec:apporach}
To tackle the task, we chose to split it in two different steps.
First we have to train the model to separate between the known classes ($\mathcal{C}_s$) and the unknown classes ($\mathcal{C}_{t\setminus s})$ in a reliable enough way.
This is achieved by using a semi-supervised task, by training to model to both recognize a sample class and its correct orientation.
The second step is similar to a classic CSDA problem, where we train the model on a union of both source and target datasets.

\subsection{Rotation Recognition}

Let's denote with $rot({\bf x}, i)$ the rotation of the sample image {\bf x} by $i\times 90$ degrees clockwise.
This is the self-supervised part of our proposed model as rotations $i \in [0, 3]$ can be randomly generated and then predicted.
To avoid situations where the objective orientation of a sample would be a too complicated task, even for a human being, we also feed in input to the model the un-rotated image features.
Alternatively, instead to have the model predict the rotation of a sample for any class, we also try using a different head for each one of the known classes $\mathcal{C}_s$, along with different loss functions.

\subsection{Step I: Sample Separation}

To separate samples we train the model on an enhanced version of $\mathcal{D}_s$, $\tilde{\mathcal{D}_s} = \{({\bf x}_i^s, {\bf \tilde x}_i^s, z_i^s)\}_{i=1}^{N_s}$ where ${\bf \tilde x}_i^s$ is the rotated version of ${\bf x}_i^s$ and $z_i^s$ is the rotation index associated to image $i$.
The network is composed by an extractor $E$ and two classifiers, $R_1$ and $C_1$ in its standard form.
When using a multi-head rotation classifier, it is composed of $|\mathcal{C}_s|+1$ heads for the rotation task, and an additional one for the classification task.
When using a multi-head predictor, which head to use during separation is chosen by the object classifier $C_1$.
The features of both original and rotated image are used to predict the rotation as ${\bf \tilde z}_s = softmax(R_1([E({\bf x}^s), E({\bf\tilde x}^s)]))$ in the single-head case and using the $j$-th head as ${\bf\tilde{z}}_s = softmax(R_{1,j}[E({\bf x}^s), E({\bf\tilde{x}}^s)])$ in the multi-head case.
Classes are predicted only using un-rotated features as $\tilde{\bf y}^s = softmax(C_1(E({\bf x}_i^s)))$.
The model is training by minimizing an objective function defined as $\mathcal{L}_1 = \mathcal{L}_{C_1} + \mathcal{L}_{R_1}$.
This is the sum of two cross-entropy loss functions as in:

\begin{equation}
  \mathcal{L}_{C_1} = -\sum_{i\in\mathcal{D}_s} {\bf y}_i^s \log \tilde{\bf y}_i^s
  \label{eq:baseloss_class}
\end{equation}

\begin{equation}
  \mathcal{L}_{R_1} = -\alpha_1\sum_{i\in\mathcal{\tilde{D}}_s} {\bf y}_i^s \log \tilde{{\bf z}}_i^s
  \label{eq:baseloss_rot}
\end{equation}

Where $\alpha_1$ is a weight associated to the rotation task.
We also try using an extended rotation objective function $\mathcal{L}_{R_1}^*$ also implementing a center loss function\cite{CenterLoss}:

\begin{equation}
  \mathcal{L}_{R_1}^* = -\alpha_1\sum_{i\in\mathcal{D}_s} {\bf y}_i^s \log \tilde{{\bf z}}_i^s+\lambda_1||{\bf v}_i^s-\gamma({\bf z}_i^s)||_2^2
  \label{eq:center_loss}
\end{equation}

Here {\bf $v_i$} is the penultimate layer of the rotation classifier, called {\it features}, and $\gamma({\bf z}_i)$ gives the center of the features ${\bf v}_i$ associated to class $i$ and $||\cdot||_2^2$ is the $l$-2 norm and $\lambda_1$ is the weight associated with this extension of the loss function.

When training is completed, we can start separating samples.
To do so, we get the normality score $\mathcal{N}(\cdot)$ which is defined as the maximum prediction of the rotation classifier, $\mathcal{N}({\bf\tilde{x}}_i^s) = \max({\bf\tilde{z}}_i^t)$.
To tell wheter a sample belongs to the known samples of the target domain $\mathcal{D}_t^{knw}$ or the unknown one $\mathcal{D}_t^{unk}$ requires choosing a threshold $\tilde{\mathcal{N}}$.
Then we can simply separate as:

\begin{equation}
  \begin{cases}
    {\bf\tilde x}_i^t \in \mathcal{D}_t^{knw} & \text{ if } \mathcal{N}({\bf\tilde x}_i^s) \geq \tilde{\mathcal{N}} \\
    {\bf\tilde x}_i^t \in \mathcal{D}_t^{unk} & \text{ if } \mathcal{N}({\bf\tilde x}_i^s) < \tilde{\mathcal{N}}
  \end{cases}
  \label{eq:sample_separation}
\end{equation}

When employing a multi-head architecture, we need to choose which one of the $|\mathcal{C}_s|$ heads to use for the classification task.
Head $R_{1,j}$ is used by picking $j$ as $\displaystyle j = \arg\max_j {\bf\tilde{y}}_i^t$.

\subsection{Step II: Domain Alignment}
\label{sec:domain_alignment}

In this phase, two "new" datasets are used.
The first one is $\mathcal{D}_s^*$, composed as $\mathcal{D}_s \cup \mathcal{D}_t^{unk}$.
Note that since $\mathcal{D}_t$ is an unlabeled dataset but $\mathcal{D}_s^*$ is a labeled one, all samples in $\mathcal{D}_t^{unk}$ are simply labeled as unknowns.
The second one is $\mathcal{D}_t^{knw}$ which is still an unlabeled dataset.
Here we also introduce two new classifiers, $C_2$ and $R_2$.
$C_2$ is the object classifier, but while $C_1$ had a $|\mathcal{C}_s|$-dimensional output, having to accomodate the new "Unknown" class, it has a $|\mathcal{C}_s|+1$-dimensional output.
$R_2$ instead is the rotation classifier, which in this step is always single-headed and has its own weight $\alpha_2$.
These two classifiers try to minimize the objective function described in equations \ref{eq:baseloss_class} and \ref{eq:baseloss_rot}.
One of the main differences between the two steps is that while in the first one we used $C_1$ to regularize the rotation task, here we use $R_2$ to regularize the main classification task.

\subsection{Performance Metrics}
\label{sec:performance_metrics}

Evaluating model perfomance requires finding a balance between two values:
{\it OS*}, the share of correctly classified samples;
{\it UNK}, the share of correclty rejected samples.
A model never confident enough to reject a sample as unknown could still achieve high {\it OS*} scores and near-zero {\it UNK}, while a model marking every given sample as unkwnown will achieve perfect {\it UNK} and zero {\it OS*}.
To compare models in a robust manner, we picked an harmonic mean between {\it OS*} and {\it UNK}, defined as $HOS = 2\frac{OS^*\times UNK}{OS^*+UNK}$.
This type of mean is more biased towards the lowest of the two scores, resulting in a more severe evaluation of models.

\section{Experiments}
\label{sec:experiments}

\subsection{Dataset}
\label{sec:dataset}

Our model is tested on the {\it Office-Home} dataset\cite{OfficeHomeDataset}, which features $65$ classes of images over four different domains: Art (A), Clipart (C), Product (P) and Real World (R).
We set the first $45$ classes to be known while the remaining $20$ are unknown.
After each epoch performed during the domain alignment phase, a validation run is performed on the entire original target dataset.
For each experiment, we report both the best achieved HOS and the last reported HOS.
As separation is crucial for the model effectiveness, we also report the computer AUROC score for the first part.

\subsection{Results}
\label{sec:results}
Table \ref{tab:results} contains the results for all 12 (ne abbiamo provati davvero 12???) possible domain shifts, as well as perfomance differences when using multi-head rotation classifiers and center loss.
Each mode has a specific configuration further discussed in \ref{sec:ablation_study}.

\begin{table}
  \centering
  \begin{tabular}[t]{||c|c|c||c|c||}
    \hline
    \multicolumn{5}{||c||}{Single-Head, CE Loss} \\
    \hline
    Source & Target & AUROC & HOS & HOS$_{Best}$ \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Multi-Head, CE Loss} \\
    \hline
    Source & Target & AUROC & HOS & HOS$_{Best}$ \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Single-Head, CE+C Loss} \\
    \hline
    Source & Target & AUROC & HOS & HOS$_{Best}$ \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Multi-Head, CE+C Loss} \\
    \hline
    Source & Target & AUROC & HOS & HOS$_{Best}$ \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    \hline 
    \hline
    \multicolumn{5}{||c||}{No Rotation} \\
    \hline
    Source & Target & AUROC & HOS & HOS$_{Best}$ \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    S & T & 50\% & 30\% & 30\% \\
    \hline
  \end{tabular}
  \caption{\label{tab:results}Test Caption}
\end{table}

\section{Implementation Details}
\subsection{Model Parameters}
\label{sec:model_parameters}

All models were run using a Stochastic Gradient Descent with batch size 32, a weight decay of 0.0005 and momentum set to 0.9.
For single-headed models, the base learning rate is set to 0.001 and for multi-headed models it is set to 0.003.
This is because multi-headed models take a longer time to converge.
All rotation classifier have the learning rate set to a tenth of the base learning rate.
Learning rates are reduced by a factor of 10 after 90\% of epochs.
All models are run for 50 epochs for the first step described in \ref{eq:sample_separation} and for 25 epochs for the second step described in \ref{sec:domain_alignment}.
For model using the center loss, a centroid learning rate of 0.5 is used.
Other parameters are configuration-sepcific and reported in table \ref{tab:params}.
When considering a "No Rotation" model, it is meant that $\alpha_2 = 0$ is used.


\begin{table}
  \centering
  \begin{tabular}[b]{||c|c|c|c|c|c||}
    \hline
    MH & CL & $\alpha_1$ & $\alpha_2$ & $\lambda_1$ & $\mathcal{\tilde N}$ \\
    \hline
    Off & Off & 0.0 & 0.0 & 0.0 & 0.0 \\
    On & Off & 0.0 & 0.0 & 0.0 & 0.0 \\
    Off & On & 0.0 & 0.0 & 0.0 & 0.0 \\
    On & On & 0.0 & 0.0 & 0.0 & 0.0 \\
    \hline
  \end{tabular}
  \caption{\label{tab:params}Model Parameters}
\end{table}

\subsection{Ablation Study}
\label{sec:ablation_study}
As shown by table \ref{tab:results}, different architectures greatly improve performance, and as shown by table \ref{tab:params} each one has its set of optimal parameters.
To choose the best parameters, a sequential approach is followed, by optimizing one parameter at the time.
The order of optimization followed is: $\alpha_1$, $\lambda_1$, $\mathcal{\tilde N}$, $\alpha_2$.
Models not using center loss skip the $\lambda_1$ optimization.
Ablation study were run on two domain shift, Art $\to$ Clipart and Clipart $\to$ Art.
In table \ref{tab:ablation_params} we report the steps followed, picking the parameter that gives the highest mean HOS between the two shifts.
All other settings follow what is reported in \ref{sec:model_parameters}.
For compactness, here are the final results of the data otherwise reported from \ref{tab:ablationoffoff} on.


\begin{table}
  \centering
  \begin{tabular}[h]{||c|c|c|c||c||}
    \hline
    \multicolumn{5}{||c||}{MH OFF CL OFF} \\
    \hline
    $\alpha_1$ & $\mathcal{\tilde N}$ & $\alpha_2$  & AUROC & HOS$_{mean}$\\    
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\alpha_1$} \\
    \hline
    1.0 & \multirow{4}{*}{0.5} & \multirow{4}{*}{3.0} & 0.5 & 30\% \\
    3.0 & & & 0.5 & 30\% \\
    5.0 & & & 0.5 & 30\% \\
    10.0 & & & 0.5 & 30\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\mathcal{\tilde N}$} \\
    \hline
    \multirow{4}{*}{1.0} & 0.40 & \multirow{4}{*}{3.0} & 0.5 & 50\%\\
    & 0.50 && 0.5 & 50\% \\
    & 0.55 && 0.5 & 50\% \\
    & 0.60 && 0.5 & 50\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\alpha_2$} \\
    \hline
    \multirow{4}{*}{1.0} & \multirow{4}{*}{0.6} & 1.0 & 0.5 & 50\%\\
    && 3.0 & 0.5 & 50\% \\
    && 5.0 & 0.5 & 50\% \\
    && 10.0 & 0.5 & 50\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Final Parameters} \\
    \hline
    1.0 & 0.6 & 3.0 & 0.5 & 50\%\\
    \hline
  \end{tabular}
  \caption{\label{tab:ablationoffoff} Ablation performed for OFF-OFF configuration}
\end{table}
\section{Future Work}

The results show that self-supervision technique can help in domain adaptation task and open-set classification tasks.
A few critical points still remain on our method of study and proposed solution.
The most important one is probably parameter choosing for different models, as we have seen that variations cause huge differences in results and sequential optimization of parameter is a sub-optimal heuristic.
Furthermore, having the model to learn two different tasks at once, it could be useful to use a slower learning model at the expense of longer training times.

%%%%%%%%% REFERENCES
{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
