% CVPR 2022 Paper Template
% based on the CVPR template provided by Ming-Ming Cheng (https://github.com/MCG-NKU/CVPR_Template)
% modified and extended by Stefan Roth (stefan.roth@NOSPAMtu-darmstadt.de)

\documentclass[10pt,twocolumn,letterpaper]{article}

%%%%%%%%% PAPER TYPE  - PLEASE UPDATE FOR FINAL VERSION
%\usepackage[review]{cvpr}      % To produce the REVIEW version
\usepackage{cvpr}              % To produce the CAMERA-READY version
%\usepackage[pagenumbers]{cvpr} % To force page numbers, e.g. for an arXiv version

% Include other packages here, before hyperref.
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{caption}
% It is strongly recommended to use hyperref, especially for the review version.
% hyperref with option pagebackref eases the reviewers' job.
% Please disable hyperref *only* if you encounter grave issues, e.g. with the
% file validation for the camera-ready version.
%
% If you comment hyperref and then uncomment it, you should delete
% ReviewTempalte.aux before re-running LaTeX.
% (Or just hit 'q' on the first LaTeX run, let it finish, and you
%  should be clear).
\usepackage[pagebackref,breaklinks,colorlinks]{hyperref}
\usepackage{multirow}
\usepackage{float}
\usepackage{url}


% Support for easy cross-referencing
\usepackage[capitalize]{cleveref}
\crefname{section}{Sec.}{Secs.}
\Crefname{section}{Section}{Sections}
\Crefname{table}{Table}{Tables}
\crefname{table}{Tab.}{Tabs.}


%%%%%%%%% PAPER ID  - PLEASE UPDATE
\def\cvprPaperID{*****} % *** Enter the CVPR Paper ID here
\def\confName{CVPR}
\def\confYear{2022}

\begin{document}

%%%%%%%%% TITLE - PLEASE UPDATE
\title{Open-Set Domain Adaptation through Self-Supervision}

\author{Protopapa Andrea, Quarta Matteo, Ruggeri Giuseppe, Versace Alessandro\\
Politecnico di Torino\\
Italy\\
{\tt\small s\{286302,292477,292459,292435\}@studenti.polito.it}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}
\maketitle

%%%%%%%%% ABSTRACT
\begin{abstract}
  In machine learning applications, domain adaptation (DA) techniques try to mitigate the problem of having different domains in the training and test data.
  Another common problem is represented by the presence of more semantic classes in the test data, which are unknown
  and completely new to the developed models. The latter problem comes under the name of novelty or anomaly detection.
  In real world scenarios, it is becoming extremely common suffering of both problems.
  \textit{Open-Set Domain Adaptation (OSDA)} methods try to tackle these problems by jointly adapting a model trained on a labeled source domain to an unlabeled target domain
  while performing novelty detection. We propose a new method leveraging a self-supervised technique, rotation recognition, constisting in first performing
  novelty detection on the target data and then aligning the two domains avoiding potential negative adaptation.
  Furthermore, we assess the performance using a new metric which represents in a balanced way the ability to jointly solve the two problems.
  Experiments conducted on the Office-Home benchmark show interesting results and method effectiveness.
  Full code for our proposed method is made publicly available \url{https://github.com/coccocarmiano/AML2021}
\end{abstract}

\section{Introduction}
\label{sec:intro}
Nowadays, the widespread usage of deep neural networks to accomplish computer vision tasks has brought huge benefits.
In real world applications, as the tasks to cope with are becoming more and more challenging, machine learning methods commonly suffer of a gap between the performance
obtained during the development, and the actual performance observed in real usages. 

One of the problems which is causing this loss of performance is the domain gap between the training data and the actual observed data. Intuitively, if we train a model on a specific domain,
such as employing real world pictures depicting real objects, for example to perform classification, we expect that the model will perform well on a fairly large variety of test cases.
However, the actual data present a huge variety on the domains while still representing the same semantic classes that we want to predict.
For example, we may want to be capable of predicting that both an image of a real elephant and a drawing of an elephant are containing the semantic class \textit{elephant}.
Domain adaptation techniques have been developing in recent years to reduce the domain gap between a labeled source domain and one or more unlabeled target domains.
Generally, this is done by enforcing the learning of domain-invariant patterns of both domains.

Another big issue,
named {\textit Open-Set Recognition (OSR)} \cite{OSRsurvey},
features the presence of additional anomalous semantic classes in the observed data and requires to both accurately classify
known observed samples and also reject the ones from unknown classes.

The joint presence and accounting of these two problems has been formalized as a new sub-field of computer vision with the name of \textit{Open-Set Domain Adaptation (OSDA)}.
As a consequence, 
if we try to reduce the domain gap between the whole target and the source domain, 
we will observe an unwanted alignment between
the data belonging to anomalous semantic classes and the source classes we want to model and predict.
For this reason, 
it is important to first perform anomaly detection of the additional set of unknown classes
and then do the alignment between the source and the target domain identified as known
translating the problem into a \textit{Closed-Set Domain Adaptation (CSDA)} one.

Common machine learning methods usually leverage huge manually annotated datasets to perform well on the given tasks.
However, 
acquiring such data is often very costly and relying on this data may not be scalable in large applications on the long run.
Recently, 
a commonly employed approach is self-supervised learning, 
which consists in creating new automatically labeled data starting from the original unlabeled data.
The fundamental idea is creating some auxiliary task from input data so that the model can learn the underlying structure of the data,
such as high-level knowledge, correlations, and metadata embedded.
This type of learning has been recently used for Domain Adaptation, 
learning robust cross-domain features and supporting generalization \cite{CarlucciJigsaw,SelfSupervisedXu},
and also for some Open Set problems specialized in anomaly detection and discriminating anomalous data \cite{bergman2020classificationbased,GeoTran}. 

The approach presented in this paper combines the power of the self-supervised learning with the standard supervised learning approach for semantic class recognition.
We propose a two stage method aiming to identify and isolate unknown class samples in the first stage, 
and reducing the domain gap between domains in the second stage to avoid negative transfer.
A schematic view of the stages can be seen in figures \ref{fig:step1}, \ref{fig:separation}, \ref{fig:step2}.
This is done by using a modified version of the rotation task as self-supervised method,
predicting the relative rotation between an image and its rotated version.
Finally, a classifier is used to predict if each target sample belongs to either one of the knwon classes or to an unknown class, 
being rejected in the latter case.
We evaluate the method on the Office-Home benchmark \cite{OfficeHomeDataset} using a new OSDA metric. 

To wrap up, our \textbf{main contributions} are: 
\begin{enumerate}
  \item we define a new method to tackle OSDA problems which exploits the rotation recognition task to perform both the known/unknown target separation
and the domain adaptation;
  \item we introduce a new OSDA metric which properly balances the measure of both the performance on predicting the known classes and the performance on doing the unknown rejection;
  \item we conduct an extensive ablation over the hyperparameters for different variants of the self-supervised task underlying the benefits of some techniques over others.
\end{enumerate}

\section{Related Work}
\label{sec:relatedwork}
\textbf{Closed-Set Domain Adaptation} is the setting where labeled training data is 
available on a source domain, but the goal is to have good performance on a 
unlabeled target domain with a different marginal distribution
of data. The aim is to align the learned representations of the source and target domains.

Methods for unsupervised domain adaptation in computer vision can be divided into 
three broad classes. The first class aims to induce alignment in some feature space optimizing the distributional discrepancy \cite{bousmalis2016domain,long2015learning,ganin2015unsupervised,hoffman2017cycada}.
The second class of methods directly transforms the source images to resemble 
the target images with generative models \cite{taigman2016unsupervised,tzeng2017adversarial,bousmalis2017unsupervised}, 
operating on image pixels directly instead of an intermediate 
representation space.
The third class uses a model trained on the labeled source data to 
estimate labels on the target data, then trains on some of those estimated 
pseudo-labels (e.g. the most confident ones), therefore bootstrapping through the 
unlabeled target data. This technique is borrowed from semi-supervised learning, 
where it is called co-training \cite{saito2017asymmetric,zou2018domain,chen2017rethinking}.

In contrast, the presented method uses a Self-Supervised approach, already presented in 
the recent years by many other works \cite{sun2019unsupervised,saito2020universal,yue2021prototypical}
using pretext tasks (e.g., image rotation, jigsaw puzzle \cite{CarlucciJigsaw}, mutual information (MI) \cite{selfsupfeng}, instance discrimination \cite{chen2020simple}) to learn high-level feature representation in 
source and target domains by jointly training a shared feature extractor.

\textbf{Open Set Recognition} (OSR) has the objective of learning a classifier 
that can reject the unknown samples while classifying the known classes accurately.

The problem was first formulated in \cite{firstPaperOSR} and since then, several other works have analyzed this challenge in the context 
of deep networks \cite{perera2019deep, hein2019relu, bendale2015open}.
Some techinques presented in the past were focus on an adversarial approach to 
delineate closed from open-set images \cite{ge2017generative, kong2021opengan}.

In the recent years different works have landed to a Self-Supervised solution, used also in the 
presented method, engaging in learning invariant representations to the 
transformations of the input data, improving separation of classes from each other 
and from open-set samples \cite{dhamija2021selfsupervised,jia2021selfsupervised,GeoTran}.

\textbf{Self-supervised Learning} is an emerging field focused on using 
data itself as supervision for auxiliary (also called “pretext”) 
tasks that learn deep feature representations, which will hopefully be informative 
for downstream “real” tasks.
Many such auxiliary tasks have been proposed in the literature, including
image colorization \cite{zhang2016colorful},
image jigsaw puzzle \cite{noroozi2017unsupervised}, geometric transformations \cite{dosovitskiy2015discriminative}.

Another geometric transformation task is rotation recognition \cite{OldROS}, where input images
are rotated by multiples of 90° and the network is trained to predict the rotation
angle of each image. This pretext task has been successfully used for both anomaly
detection \cite{GeoTran} and closed-set domain adaptation \cite{SelfSupervisedXu}.

\textbf{Open Set Domain Adaptation} (OSDA) is a challenging domain adaptation 
setting which allows the existence of unknown classes on the target domain.
The term "OSDA" was first introduced by Busto and Gall \cite{firstosda}, but the 
currently accepted definition was introduced by Saito \etal \cite{saito2018open} considering the target as 
containing all the source categories and additional set of private categories 
that should be considered unknown. They have presented Open Set
Back-Propagation (OSBP) \cite{saito2018open} as an adversarial method 
to increase the prediction variances so that the generator can choose if accept or not a target saple.
Separate To Adapt (STA) \cite{sat} train a multi-binary classifier to progressively 
separate the samples of unknown and known classes.
Universal Adaptation Network (UAN) \cite{uan}, originally proposed for the universal 
domain adaptation setting that is a superset of OSDA, quantify the sample-level 
transferability and recognize the unknown samples based on it.
Attract or Distract (AoD) \cite{feng2019attract} refine the decision by using 
metric learning to reduce the intra-class distance in known classes and push 
the unknown class away from the known classes.
Jing \etal \cite{Jing_Li_Zhu_Ding_Lu_Yang_2021} have recently proposed a method
to recognize the unknown samples according to the centroid deviation angles and
employing the statistical Extreme Value Theory to recognize the unknown 
samples that are misclassified into known classes.

The approach that we are going to present is instead based on a Self-Supervision 
task, the rotation recognition, used to both separate known and unknown target 
samples and align the known source and target distributions.

\section{Method}
\label{sec:method}

\subsection{Problem Formulation}
\label{sec:problemformulation}
We define as $\mathcal{D}_s = \{({\bf x}_i^s, y_i^s)\}_{i=1}^{N_s} \sim p_s$ the source dataset whose distribution of samples and labels is $p_s$,
while $\mathcal{D}_t = \{{\bf x}_i^t\}_{i=1}^{N_t} \sim p_t$ is the unlabeled target dataset drawn from distribution $p_t$. 

The source dataset $\mathcal{D}_s$ is associated with a set of known classes $\mathcal{C}_s$,
whereas the target dataset $\mathcal{D}_t$ contains a set of classes $\mathcal{C}_t = \mathcal{C}_s \cup \mathcal{C}_{t \setminus s}$.
In other words,
$|\mathcal{C}_s| < |\mathcal{C}_t|$ and $\mathcal{C}_s \subset \mathcal{C}_t$. 

In OSDA we have that $p_s \neq p_t$.
Moreover, it holds that $p_s \neq p_t^{\mathcal{C}_s}$, where $p_t^{\mathcal{C}_s}$ denotes the distribution of the target domain if we restrict to
the shared classes $\mathcal{C}_s$. 

Summarizing, in OSDA tasks, we have both a domain gap ($p_s \neq p_t^{\mathcal{C}_s}$)
and a category gap ($\mathcal{C}_s \neq \mathcal{C}_t$). Moreover, the goal is to assign the target samples either to a category ${i \in \mathcal{C}_s}$,
or to reject them as {\it unknown}.
A metric to measure the complexity of an OSDA problem is the {\it openness} betweeen the source and the target domain \cite{bendale2015open}, 
defined as $\displaystyle \mathbb{O} = 1-\frac{\mathcal{C}_s}{\mathcal{C}_t}$.
When $\mathbb{O} > 0$, we are dealing with an OSDA problem, otherwise we are in a CSDA setting.

\subsection{Approach}
\label{sec:apporach}
The proposed method is split in two sequential stages. First, to avoid negative transfer during the domain alignment step, we want a model
that is able to separate the target dataset into $\mathcal{D}_t^{unk}$, which contains images belonging to unknown classes,
and $\mathcal{D}_t^{knw}$, which contains only images belonging to the known classes. To do that, we leverage the
power of the rotation pre-text task to perform the separation.
Next, in the second stage, we can close the gap between the source domain and the target domain exploiting $\mathcal{D}_t^{knw}$
using the same self-supervised task. Furthermore, we leverage $\mathcal{D}_t^{unk}$ to learn the additional {\it unknown} class.

\subsection{Rotation Recognition}
\label{sec:rotrecognition}

We denote with $rot({\bf x}, k)$ the rotating function of a sample image {\bf x} by $k\times 90$ degrees clockwise.
The self-supervised pre-text task consists in generating a random rotation index $k \in [0, 3]$ that then becomes
the label for the rotated version of the image ${\bf \tilde{x}} = rot({\bf x}, k)$.
Then, the task becomes a standard classification of the correct rotation index ($\mathcal{C}_r = \{0, 1, 2, 3\}$).

{\it Relative orientation:}
more precisely, we exploit a relative rotation task,
which implies that both the features of the original and rotated image are supplied to the rotation classifier.
This is preferred over the absolute rotation task as some objects might not have an absolute coherent orientation
inside the dataset
(e.g. a pen may be present in different rotated versions inside the dataset). 

{\it Multi-head rotation classifier:}
alternatively, 
instead of having a single rotation head predicting the rotation of a sample regardless of its semantic class, 
we also try using a different head for each known class $\in \mathcal{C}_s$, each one responsible of predicting the rotation
of the images belonging to that semantic class.
This variation can mitigate the problem of trying to predict the rotation of a larger number
of semantic classes. Infact, as the number of semantic classes grows,
the problem of predicting the relative orientation becomes more difficult.

The application of the rotation recognition pre-text task allows to effectively favor and force the model to learn
domain-independent patterns, which are crucial to perform the novelty detection in a cross-domain fashion and, moreover,
to successfully perform the domain alignment.
To provide an explanation of why this applies, we can think that a rotation classifier needs to focus on discriminative patterns to
successfully perform the rotation predictions, such as shapes, edges, and high-level object relative position like the position of the eyes
w.r.t nose.
The method and its effectiveness is further illustrated in \cite{OldROS},
and further possible improvements will be discussed in section \ref{sec:future_work}.

\subsection{Step I: target known/unknown separation}
\label{sec:stepone}

\begin{figure}
  \includegraphics[trim= 0cm 0cm 0cm 0cm, clip, width=\linewidth]{scheme_step1.png}
  \caption{\label{fig:step1} Schema of the learning process for the model in Step I.}
\end{figure}

To perform the target separation we train a CNN iterating on $\tilde{\mathcal{D}_s} = \{({\bf x}_i^s, {\bf \tilde x}_i^s, z_i^s)\}_{i=1}^{N_s}$,
where ${\bf \tilde x}_i^s$ is the $z_i^s\times 90$ degrees rotated version of ${\bf x}_i^s$.
The CNN is made of a feature extractor $E$ and two heads: $R_1$ and $C_1$.
$C_1$ is the object classifier, which assigns to image ${\bf x}_i^s$ a predicted semantic class label,
while $R_1$ is the relative rotation classifier, which assigns to the rotated image ${\bf \tilde x}_i^s$ a predicted rotation label.
To keep the notation clear, we define as ${\bf y_i}$ and ${\bf z_i}$ the one-hot vector representations of the corresponding scalar labels.
Notice that the multi-head rotation classifier internally uses $|\mathcal{C}_s|$ different heads for the rotation task.
In this case, the head selected to perform the rotation prediction is up to the object classifier $C_1$ (during inference and not during training). 

The object class vector of predicted probabilities is computed as ${\bf \hat{y}_i^s} = softmax(C_1(E({\bf x_i^s})))$, while the vector of predicted probabilities
for rotation label is computed from the stacked features of the original and rotated image ${\bf \hat{z}_i^s} = softmax(R_1([E({\bf x_i^s}), E({\bf \tilde{x}_i^s})]))$.
The model is trained to minimize the objective function $\mathcal{L}_1 = \mathcal{L}_{C_1} + \mathcal{L}_{R_1}$.
This is the sum of two cross-entropy loss functions:

\begin{equation}
  \mathcal{L}_{C_1} = -\sum_{i\in\mathcal{D}_s} {\bf y}_i^s \log \hat{\bf y}_i^s
  \label{eq:baseloss_class}
\end{equation}

\begin{equation}
  \mathcal{L}_{R_1} = -\alpha_1\sum_{i\in\mathcal{\tilde{D}}_s} {\bf z}_i^s \log \hat{{\bf z}}_i^s
  \label{eq:baseloss_rot}
\end{equation}

Where $\alpha_1$ is a weight associated to the rotation task.
We also try using an extended rotation loss function $\mathcal{L}_{R_1}^*$ implementing an additional center loss\cite{CenterLoss} term:

\begin{equation}
  \mathcal{L}_{R_1}^* = \sum_{i\in\mathcal{\tilde{D}}_s} -\alpha_1{\bf z}_i^s \log \hat{{\bf z}}_i^s+\lambda||{\bf v}_i^s-\gamma({\bf z}_i^s)||_2^2
  \label{eq:center_loss}
\end{equation}

Here ${\bf v_i}$ is the output of the penultimate layer of $R_1$,
$\gamma({\bf z}_i)$ is the centroid of the features associated to class $i$
(notice that the centroid is relative to a different rotation class $i$ in the multi-head variant),
$||\cdot||_2^2$ is the $l$-2 norm and $\lambda$ is the weight associated with the center loss term.
The additional center loss contribution should favor having hidden features which have
a smaller intra-class variance, resulting in a more discriminative representation
when combined with the standard cross-entropy loss,
which in contrast tries to enlarge the distance between the means of the classes.

When training is completed, 
we can start separating the target samples into known and unknown.
To do so,
{\it normality scores} $\mathcal{N}(\cdot)$ are used,
defined as the maximum prediction of the rotation classifier:
$\mathcal{N}({\bf\tilde{x}}_i) = \max({\bf\hat{z}}_i)$.
Notice that, for each target sample, all the four rotations are applied and the resulting normality score for the sample
is computed as the mean of the four normality scores.
To decide if a target sample belongs to a known semantic class or not, we compare the normality score with a threshold $\tilde{\mathcal{N}}$.

\begin{equation}
  \begin{cases}
    {\bf x}_i^t \in \mathcal{D}_t^{knw} & \text{ if } \mathcal{N}({\bf\tilde x}_i^t) \geq \tilde{\mathcal{N}} \\
    {\bf x}_i^t \in \mathcal{D}_t^{unk} & \text{ if } \mathcal{N}({\bf\tilde x}_i^t) < \tilde{\mathcal{N}}
  \end{cases}
  \label{eq:sample_separation}
\end{equation}

When using a multi-head rotation classifier, 
it is required to choose among the $|\mathcal{C}_s|$ possible heads to make the prediction.
Head $R_{1,j}$ is used where $\displaystyle j = \arg\max_j \{{\bf\hat{y}}_{i, [j]}^t\}_{j=0}^{|\mathcal{C}_s|-1}$
($j$ is the component $j$ of the vector).

The key idea behind the normality score is that, if $R_1$ is confident enough on its predicted rotation,
it is likely that it has managed to successfully recognize the rotation.
Since $R_1$ has learned the domain-independent patterns of the known classes from the source dataset up to this point,
it should be able to recognize the rotations applied only to images belonging to such classes.

\subsection{Step II: domain alignment}
\label{sec:domain_alignment}


\begin{figure}[t]
  \includegraphics[trim= 0cm 0cm 0cm 0cm, clip, width=\linewidth ]{scheme.png}
  \caption{\label{fig:separation}\centering Schema of the separation process into known and unkown samples on target images}
\end{figure}


In this step,
having separated the target into a known part $\mathcal{D}_t^{knw}$,
and an unknown part $\mathcal{D}_t^{unk}$,
we arrange two new datasets in order to perform the domain alignment while also learning
the unknown class.
The first one is $\mathcal{D}_s^*$, composed as $\mathcal{D}_s \cup \mathcal{D}_t^{unk}$,
which contains the original source images plus the target images identified as unknown classes.
We thus set the labels for $\mathcal{D}_t^{unk}$ as the class {\it unknown}.
The second one is $\mathcal{D}_t^{knw}$, which can be used to perform the domain alignment
without the risk of negative transfer.
While the feature extractor $E$ is inherited from the previous stage and leverages the previous training phase,
we also use two new classifiers,
$C_2$ and $R_2$.
They are similar to the previous classifiers but they have two important differences.
$C_2$ now has a $(|\mathcal{C}_s|+1)$-dimensional output to accomodate also the unknown class predictions and also
benefits from the previous learning,
while $R_2$ is always a single-head rotation classifier and starts the learning from scratch.
The training phase is the same as before with the difference that we do not have the center loss this time.
We also employ a different hyperparameter $\alpha_2$ to weigh the rotation classifier loss contribution.
The objective function is again $\mathcal{L}_2 = \mathcal{L}_{C_2} + \mathcal{L}_{R_2}$,
where the two contributions are identical to equations \ref{eq:baseloss_class} and \ref{eq:baseloss_rot}.
We report the $\mathcal{L}_{R_2}$ loss contribution to make clear the usage of $\alpha_2$ and we
recall that $\mathcal{L}_{C_2}$ is now computed using the new arranged dataset $\mathcal{D}_s^*$.

\begin{figure}[ht]
  \includegraphics[trim= 5.2cm 0cm 2cm 0cm, clip, width=\linewidth ]{scheme_step2.png}
  \caption{\label{fig:step2}\centering Schema of the learning process of the model in Step II}
\end{figure}


\begin{equation}
  \mathcal{L}_{R_2} = -\alpha_2\sum_{i\in\mathcal{D}_t^{knw}} {\bf z}_i^s \log \hat{{\bf z}}_i^s
  \label{eq:baseloss_rot2}
\end{equation}

\subsection{Performance metrics}
\label{sec:performance_metrics}

To have a meaningful comparison of the effectiveness
of the models to solve OSDA problems, we need to resort to a metric which
correctly measures the goodness of our predictions.
The difficulty lies on the joint presence of two, usually conflicting, goals.
We recall that an OSDA problem is characterized by the concurrent presence of
a domain difference and a class set difference.
The final goal is to correctly classify the samples belonging to known classes
while rejecting the samples of unknown classes.
The performance of both the goals are typically taken into account by two metrics, {\it OS*},
which is the fraction of the correctly classified samples belonging to known classes, and {\it UNK},
which in contrast is the accuracy over the correctly rejected samples.
Since the goals are usually conflicting, we need an overall metric which is able to correctly balance
the two contributions, providing low values if at least one of the two is low.
We therefore pick the harmonic mean of the two contributions, defined as $HOS = 2\frac{OS^*\times UNK}{OS^*+UNK}$.
We underline that $HOS$ is a more severe and fair metric to effectively measure the goodness of a model to perform well
on a given OSDA problem because, from the definition of the harmonic mean, it tends to give a bigger weight to the smaller values.

Lastly, we also exploit the AUROC metric to assess the goodness of the system in separating
the target into a known and unknown part.

\section{Experiments}
\label{sec:experiments}

\subsection{Benchmark}
\label{sec:dataset}

We evaluate the effectiveness of our proposed method exploiting the {\it Office-Home} \cite{OfficeHomeDataset} benchmark dataset,
widely used to assess the performance for OSDA settings.
The dataset is characterized by four different domains: Art (A), Clipart (C), Product (P) and Real World (R), and
each domain contains the same 65 object class categories.
We underline that the large domain gaps and the number of classes make the benchmark challenging.
For the experiments, we fix the first 45 classes (in alphabetical order) as the known classes ($\mathcal{C}_s$),
and the remaining 20 classes as the unknown ones ($\mathcal{C}_{t \setminus s}$).
For each experiment, we collect the HOS value achieved after the last training epoch of the step 2,
along with the AUROC value computed after the step 1, crucial to assess the goodness
of the system's components responsible for the separation of the target.

\subsection{Implementation Details}
\label{sec:implementation_details}

As we have already seen in section \ref{sec:method}, the network is composed of a feature extractor $E$, and two heads, $C_{1/2}$ and $R_{1/2}$.
For the feature extraction, we employ a ResNet-18\cite{ResNet18} pre-trained on ImageNet.
All the experiments are run training with a batch size equals to 32 and using a stochastic gradient descent optimization strategy
with a weight decay of 0.0005 and a momentum of 0.9.
The learning rate is set to 0.001 in the cases where we use a single-head rotation classifier as $R_1$, and 0.003 in the multi-head case.
The reason of this choice lies on the fact that, with a multi-head rotation classifier, we have to train 45 heads instead of a single one,
so the training process needs to be speeded up.
Moreover, we set a learning rate 10 times lower for the weights of the feature extractor and $C_{1/2}$.
We also decrease the learning rate using a step learning rate scheduler, which reduces it by a factor of 10 after 90\% of total epochs.
All models are trained for 50 epochs for the step one and for 25 epochs for the step two.
For models using the center loss, a centroid learning rate of 0.001 is used.
The other hyperparameters are analyzed performing an ablation on them,
details can be found on the section \ref{sec:ablation_study} and the results of the ablation are
reported on appendix \ref{sec:tables}.

{\it Feature extractor E}: ResNet-18 pre-trained on ImageNet. It is also shared between step 1 and step 2.

{\it Classifier $C_1$}:
It is a simple linear classifier with a 45-dimensional output.

{\it Classifier $C_2$}:
Like $C_1$ but with a 46-dimensional output to consider also the unknown class. Furthermore, it leverages the learning already performed on $C_1$.
It is worth noting that we rebalance the weight of the loss computed on samples belonging to the class unknown in the following way.
After having separated the target into $\mathcal{D}_t^{knw}$ and $\mathcal{D}_t^{unk}$,
we select the weight as $\frac{\mathcal{\bar{D}}_s}{|\mathcal{D}_t^{unk}|}$,
where ${\mathcal{\bar{D}}_s}$ is the average number of samples contained in the source known classes.
This ensures that we correctly balance the unknown class during training in a way which is independent
on the number of identified target unknown samples during the separation.

{\it Classifier $R_1$, $R_2$}: 
they are seen as discriminators composed by a linear layer ($1024\to 256$),
followed by a batch normalization layer and a leaky relu activation.
The last layer is then a linear one for the actual classification ($256\to 4$).
In the case of multi-head rotation classifier,
the discriminator is then composed of $|\mathcal{C}_s|$ different linear layers, both for the first and the last one.
Moreover, the 256-dimensional output are the features used by the center loss for inferring class centroids.
$R_2$ is always a single-head discriminator, even if using the multi-head variant.

\subsection{Results}
\label{sec:results}
\begin{figure}[!htb]
  \centering
  \begin{subfigure}[!htb]{0.35\textwidth}
    \includegraphics[trim=0 0 0 2cm, clip, width=\linewidth]{hos-a1.png}
  \end{subfigure}
  \begin{subfigure}[!htb]{0.35\textwidth}
    \includegraphics[trim=0 0 0 2cm, clip, width=\linewidth]{hos-a2.png}
  \end{subfigure}
  \begin{subfigure}[!htb]{0.35\textwidth}
    \includegraphics[trim=0 0 0 2cm, clip, width=\linewidth]{hos-l.png}
  \end{subfigure}
  \begin{subfigure}[!htb]{0.35\textwidth}
    \includegraphics[trim=0 0 0 2cm, clip, width=\linewidth]{hos-th.png}
  \end{subfigure}
  \caption{\centering\label{fig:hyperparams} HOS values for different hyperparameters. For full details see appendix \ref{sec:tables}}
\end{figure}

Table \ref{tab:results} contains the results for all 12 available domain shifts,
as well as perfomance differences when using multi-head rotation classifiers and center loss,
as described in the previous section.
Multi-headed models are more capable of correctly discriminating between known and unknown samples,
ultimately resulting in better domain alignment.
This effect is amplified when also using center loss.
AUROC and HOS values are taken at the end of the relative stages. 

\subsection{Ablation Study}
\label{sec:ablation_study}
Image \ref{fig:hyperparams} summarizes the parameter optimization process where a sequential approach is followed.
For each configuration one hyperparameter is optimized at a time,
following the order
$\alpha_1$, $\lambda_1$, $\mathcal{\tilde N}$, $\alpha_2$ and picking the parameter giving the highest HOS.
Models not using center loss just skip the $\lambda_1$ optimization.
Ablation study were run and averaged on two domain shifts, 
Art $\to$ Clipart and Product $\to$ Clipart.
In appendix \ref{sec:tables} tables \ref{tab:ablationoffoff}, \ref{tab:ablationonoff} and \ref{tab:ablationonon} report all the steps followed for each configuration.
This information is also summarized in figure \ref{fig:hyperparams}.
We can notice that most of the optimal parameters are shared between configurations,
an exception being the normality score $\mathcal{\tilde N}$ on a single-headed configuration.
This is due to the fact that having to use only one head for all classes it is better to be very confident about classifying a sample as known.
Smaller $\alpha_1$ and $\alpha_2$ values are preferred as they avoid overfitting the rotation classifiers as the object classifiers take a longer time to converge.
As center loss contributions are directly summed to the rotation classifier loss term while being much bigger, 
much smaller values need to be used for a correct model convergence.


\section{Future Work and conclusions}
\label{sec:future_work}

The results show that self-supervision technique can help in domain adaptation task and open-set classification tasks,
but some critical points still remain in our method of study and proposed solution.
The most important one is probably the parameter optimization method for different models,
as small variations can cause huge differences in results sequentially optimizing hyperparameters is a sub-optimal heuristic.
Furthermore,
having models learn different tasks simultaneosly can lead to instability, 
so using slower learning models at the expense of longer training times can improve results.
A criticality is represented by the normality score hyperparameter:
model confidence over accepting or rejecting a sample is very thight between known and unknwon samples,
so a better function do have more spread scores can bring more efficient separation,
ultimately resulting in a better quality input data in the second step.
Last,
center loss function relies on learning centroids based on sample features to predict rotation.
This formulation requires picking a specific learning rate for such centroids,
but we didn't run any test on how such learning rate impacts predictions or sample separability.

%%%%%%%%% REFERENCES
{\small
\nocite{BucciROS}
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\appendix
\section{Tables}
\label{sec:tables}

This section contains tables \ref{tab:ablationoffoff}, \ref{tab:ablationonoff} and \ref{tab:ablationonon} report the hyperparameters optimization sequence, while tables \ref{tab:results} report the final values for each domain shift with the chosen hyperparameters.

\begin{table}[h!]
  \centering
  \begin{tabular}[h]{||c|c|c||c||}
    \hline
    \multicolumn{4}{||c||}{OFF-OFF} \\
    \hline
    $\alpha_1$ & $\mathcal{\tilde N}$ & $\alpha_2$ & HOS$_{mean}$\\    
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\alpha_1$} \\
    \hline
    {\bf 1.0}  & \multirow{4}{*}{0.6} & \multirow{4}{*}{3.0} & {\bf 40.27\%} \\
    3.0  & & & 22.69\% \\
    5.0  & & & 18.78\% \\
    10.0 & & & 9.99\% \\
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\mathcal{\tilde N}$} \\
    \hline
    \multirow{6}{*}{1.0} & 0.50 & \multirow{6}{*}{3.0}  & 28.05\%\\
    & 0.55 & & 34.73\% \\
    & 0.60 & & 40.27\% \\
    & 0.65 & & 43.16\% \\
    & {\bf 0.70} & & {\bf 44.38\%} \\
    & 0.75 & & 43.87\% \\
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\alpha_2$} \\
    \hline
    \multirow{7}{*}{1.0} & \multirow{7}{*}{0.7} & 0.1 & 42.79\%\\
    && 1.0 & 44.35\% \\
    && 2.0 & 43.53\% \\
    && {\bf 3.0} & {\bf 44.38}\% \\
    && 4.0 & 43.12\% \\
    && 5.0 & 41.97\% \\
    && 10.0 & 42.01\% \\
    \hline
  \end{tabular}
  \caption{\centering\label{tab:ablationoffoff} Hyperparametr optimization process for configuration Multi-Head Off and Center-Loss Off}
\end{table}

\begin{table}[t!]
  \vspace{-2cm}
  \centering
  \begin{tabular}[h]{||c|c|c||c||}
    \hline
    \multicolumn{4}{||c||}{ON-OFF} \\
    \hline
    $\alpha_1$ & $\mathcal{\tilde N}$ & $\alpha_2$ & HOS$_{mean}$\\    
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\alpha_1$} \\
    \hline
    {\bf 1.0}  & \multirow{4}{*}{0.6} & \multirow{4}{*}{3.0} & {\bf 43.48\%} \\
    3.0  & & & 38.45\% \\
    5.0  & & & 37.61\% \\
    10.0 & & & 33.23\% \\
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\mathcal{\tilde N}$} \\
    \hline
    \multirow{6}{*}{1.0} & 0.4 & \multirow{6}{*}{3.0}  & 38.40\%\\
    & 0.45 & & 42.40\% \\
    & 0.5 & & 43.48\% \\
    & 0.55 & & 46.21\% \\
    & {\bf0.6} & & {\bf 50.17\%} \\
    & 0.65 & & 45.82\% \\
    \hline
    \hline
    \multicolumn{4}{||c||}{Picking $\alpha_2$} \\
    \hline
    \multirow{7}{*}{1.0} & \multirow{7}{*}{0.7} & 0.1 & 46.02\%\\
    && 1.0 & 47.71\% \\
    && 2.0 & 46.56\% \\
    && {\bf 3.0} & {\bf 50.17\%} \\
    && 4.0 & 46.57\% \\
    && 5.0 & 47.52\% \\
    && 10.0 & 44.56\% \\
    \hline
  \end{tabular}
  \caption{\centering\label{tab:ablationonoff} Hyperparameters optimization process for configuration Multi-Head On and Center-Loss Off}
\end{table}


\begin{table}[b!]
  \centering
  \begin{tabular}[h]{||c|c|c|c||c||}
    \hline
    \multicolumn{5}{||c||}{ON-ON} \\
    \hline
    $\alpha_1$ & $\lambda_1$ & $\mathcal{\tilde N}$ & $\alpha_2$ & HOS$_{mean}$ \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\alpha_1$} \\
    \hline
    1.0 & \multirow{4}{*}{0.01} & \multirow{4}{*}{0.05} & \multirow{4}{*}{3.0} & 44.90\% \\
    {\bf 3.0} & && & {\bf 45.33\%} \\
    5.0 & && & 42.41\% \\
    10.0 & && & 37.15\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\lambda_1$} \\
    \hline
    \multirow{5}{*}{3} & 0.0001 & \multirow{5}{*}{0.5} & \multirow{5}{*}{3.0} & 39.90\% \\
    & 0.001 & & & 36.71\% \\
    & 0.01 & & & 45.33\% \\
    & {\bf 0.05 }& & & {\bf 46.63\%} \\
    & 0.1 & & & 37.44\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\mathcal{\tilde N}$} \\
    \hline
    \multirow{5}{*}{3} & \multirow{5}{*}{0.05} & 0.4 & \multirow{5}{*}{3.0} & 44.32\% \\
    & & 0.45 & & 45.13\% \\
    & & 0.5 & & 46.63\% \\
    & & 0.55 & & 47.09\% \\
    & & {\bf 0.6} & & {\bf 49.78\%} \\
    & & 0.65 & & 47.23\% \\
    \hline
    \hline
    \multicolumn{5}{||c||}{Picking $\alpha_2$} \\
    \hline
    \multirow{7}{*}{3} & \multirow{7}{*}{0.05} & \multirow{7}{*}{0.6} & 0.1 & 45.73\% \\
    & & & 1.0 & 45.23\% \\
    & & & 2.0 & 45.76\% \\
    & & & {\bf 3.0} & {\bf 49.78\%} \\
    & & & 4.0 & 46.20\% \\
    & & & 5.0 & 48.60\% \\
    & & & 10.0 & 44.14\% \\
    \hline
  \end{tabular}
  \caption{\centering\label{tab:ablationonon} Hyperparameters optimization process for configuration Multi-Head On and Center-Loss On}
\end{table}

\begin{table*}
  \centering
  \begin{tabular}[t]{||c||c|c|c|c||c|c|c|c||c|c|c|c||}
    \hline
    \multicolumn{13}{||c||}{Results} \\
    \hline
    \multirow{2}{*}{Shift}   & \multicolumn{4}{|c||}{Single-Head, No Center Loss} & \multicolumn{4}{|c||}{Multi-Head, No Center Loss} & \multicolumn{4}{|c||}{Multi-Head, Center Loss} \\
            & AUROC & HOS   & UNK   & OS*   & AUROC & HOS   & UNK   & OS*   & AUROC & HOS   & UNK   & OS*   \\
    \hline
    A$\to$C & 52.07 & 44.76 & 58.65 & 36.19 & 57.46 & 46.64 & 62.95 & 37.04 & 59.77 & 52.68 & 62.73 & 45.40 \\
    A$\to$P & 50.84 & 51.27 & 57.56 & 46.23 & 56.31 & 54.13 & 60.42 & 49.03 & 56.86 & 52.46 & 63.35 & 44.77 \\
    A$\to$R & 27.94 & 68.13 & 17.58 & 52.25 & 59.88 & 56.60 & 53.18 & 60.65 & 58.86 & 61.67 & 66.82 & 57.26 \\
    C$\to$A & 49.52 & 41.74 & 43.10 & 40.47 & 55.28 & 42.63 & 51.57 & 36.33 & 55.76 & 46.84 & 60.19 & 38.35 \\
    C$\to$P & 54.42 & 46.99 & 41.51 & 54.15 & 59.13 & 51.88 & 60.03 & 45.69 & 58.35 & 52.69 & 62.19 & 45.72 \\
    C$\to$R & 53.43 & 46.18 & 38.64 & 57.39 & 60.75 & 56.00 & 61.29 & 51.63 & 59.15 & 55.97 & 63.18 & 50.25 \\
    P$\to$A & 48.32 & 37.07 & 33.70 & 41.20 & 55.62 & 41.72 & 48.90 & 36.39 & 53.50 & 40.97 & 49.53 & 34.94 \\
    P$\to$C & 47.76 & 43.99 & 48.42 & 40.31 & 56.77 & 46.10 & 55.11 & 39.65 & 58.64 & 46.81 & 60.95 & 37.99 \\
    P$\to$R & 50.37 & 34.14 & 22.88 & 67.27 & 59.68 & 48.41 & 40.91 & 59.30 & 61.10 & 56.22 & 56.89 & 55.58 \\
    R$\to$A & 49.39 & 25.08 & 15.99 & 58.19 & 58.47 & 45.00 & 39.50 & 52.43 & 57.98 & 48.92 & 45.61 & 52.77 \\
    R$\to$C & 49.80 & 46.70 & 48.27 & 45.23 & 57.02 & 49.00 & 42.82 & 57.34 & 57.09 & 48.69 & 61.11 & 40.47 \\
    R$\to$P & 54.10 & 23.37 & 13.89 & 73.69 & 62.33 & 50.31 & 39.51 & 69.27 & 63.96 & 59.64 & 54.40 & 66.02 \\
    \hline
    AVG     & 49.00 & 42.45 & 36.68 & 51.05 & 58.23 & 49.03 & 51.35 & 49.56 & 58.42 & 51.96 & 58.91 & 47.46 \\ 
    \hline
  \end{tabular}
  \caption{\centering\label{tab:results}Final results for the three configurations on all domain shifts. All values are percentages.}
\end{table*}


\end{document}
